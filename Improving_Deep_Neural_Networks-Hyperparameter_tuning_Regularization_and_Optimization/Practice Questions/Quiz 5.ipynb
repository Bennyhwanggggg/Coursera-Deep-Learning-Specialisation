{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Quiz - Practical aspects of deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. If you have 10,000,000 examples, how would you split the train/dev/test set?\n",
    "\n",
    "- [ ] 60% train . 20% dev . 20% test\n",
    "- [ ] 98% train . 1% dev . 1% test\n",
    "- [ ] 33% train . 33% dev . 33% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The dev and test set should:\n",
    "\n",
    "- [ ] Come from the same distribution\n",
    "- [ ] Come from different distributions\n",
    "- [ ] Be identical to each other (same (x,y) pairs)\n",
    "- [ ] Have the same number of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. If your Neural Network model seems to have high variance, what of the following would be promising things to try? (Check all that apply.)\n",
    "\n",
    "- [ ] Make the Neural Network deeper\n",
    "- [ ] Add regularization\n",
    "- [ ] Get more training data\n",
    "- [ ] Increase the number of units in each hidden layer\n",
    "- [ ] Get more test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. You are working on an automated check-out kiosk for a supermarket, and are building a classifier for apples, bananas and oranges. Suppose your classifier obtains a training set error of 0.5%, and a dev set error of 7%. Which of the following are promising things to try to improve your classifier? (Check all that apply.)\n",
    "\n",
    "- [ ] Increase the regularization parameter lambda\n",
    "- [ ] Decrease the regularization parameter lambda\n",
    "- [ ] Get more training data\n",
    "- [ ] Use a bigger neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What is weight decay?\n",
    "\n",
    "- [ ] A technique to avoid vanishing gradient by imposing a ceiling on the values of the weights.\n",
    "- [ ] Gradual corruption of the weights in the neural network if it is trained on noisy data.\n",
    "- [ ] The process of gradually decreasing the learning rate during training.\n",
    "- [ ] A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What happens when you increase the regularization hyperparameter lambda?\n",
    "\n",
    "- [ ] Weights are pushed toward becoming smaller (closer to 0)\n",
    "- [ ] Weights are pushed toward becoming bigger (further from 0)\n",
    "- [ ] Doubling lambda should roughly result in doubling the weights\n",
    "- [ ] Gradient descent taking bigger steps with each iteration (proportional to lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. With the inverted dropout technique, at test time:\n",
    "\n",
    "- [ ] You do not apply dropout (do not randomly eliminate units), but keep the 1/keep_prob factor in the calculations used in training.\n",
    "- [ ] You apply dropout (randomly eliminating units) but keep the 1/keep_prob factor in the calculations used in training.\n",
    "- [ ] You do not apply dropout (do not randomly eliminate units) and do not keep the 1/keep_prob factor in the calculations used in training\n",
    "- [ ] You apply dropout (randomly eliminating units) and do not keep the 1/keep_prob factor in the calculations used in training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Increasing the parameter keep_prob from (say) 0.5 to 0.6 will likely cause the following: (Check the two that apply)\n",
    "\n",
    "- [ ] Increasing the regularization effect\n",
    "- [ ] Reducing the regularization effect\n",
    "- [ ] Causing the neural network to end up with a higher training set error\n",
    "- [ ] Causing the neural network to end up with a lower training set error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Which of these techniques are useful for reducing variance (reducing overfitting)? (Check all that apply.)\n",
    "\n",
    "- [ ] Xavier initialization\n",
    "- [ ] Dropout\n",
    "- [ ] L2 regularization\n",
    "- [ ] Vanishing gradient\n",
    "- [ ] Data augmentation\n",
    "- [ ] Gradient Checking\n",
    "- [ ] Exploding gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Why do we normalize the inputs x?\n",
    "\n",
    "- [ ] It makes the cost function faster to optimize\n",
    "- [ ] It makes the parameter initialization faster\n",
    "- [ ] Normalization is another word for regularization--It helps to reduce variance\n",
    "- [ ] It makes it easier to visualize the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
