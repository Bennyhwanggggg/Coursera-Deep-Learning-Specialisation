{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Quiz - Optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Which notation would you use to denote the 3rd layer’s activations when the input is the 7th example from the 8th minibatch?\n",
    "\n",
    "- [ ] $a^{[8]{7}(3)}$\n",
    "- [ ] $a^{[3]{7}(8)}$\n",
    "- [ ] $a^{[8]{3}(7)}$\n",
    "- [ ] $a^{[3]{8}(7)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Which of these statements about mini-batch gradient descent do you agree with?\n",
    "\n",
    "- [ ] One iteration of mini-batch gradient descent (computing on a single mini-batch) is faster than one iteration of batch gradient descent.\n",
    "- [ ] You should implement mini-batch gradient descent without an explicit for-loop over different mini-batches, so that the algorithm processes all mini-batches at the same time (vectorization).\n",
    "- [ ] Training one epoch (one pass through the training set) using mini-batch gradient descent is faster than training one epoch using batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why is the best mini-batch size usually not `1` and not `m`, but instead something in-between? (Check all that apply.)\n",
    "\n",
    "- [ ] If the mini-batch size is `1`, you end up having to process the entire training set before making any progress.\n",
    "- [ ] If the mini-batch size is `1`, you lose the benefits of vectorization across examples in the mini-batch.\n",
    "- [ ] If the mini-batch size is `m`, you end up with batch gradient descent, which has to process the whole training set before making progress.\n",
    "- [ ] If the mini-batch size is `m`, you end up with stochastic gradient descent, which is usually slower than mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Suppose your learning algorithm’s cost $J$, plotted as a function of the number of iterations, looks like this:\n",
    "\n",
    "<img src=\"images/KIycr3grEeeJIwrF5BVsIg_f1c324824bd9220c7ee985cce1521404_cost.png\"  alt=\"Figure 1\" style=\"width: 50%;\">\n",
    "\n",
    "Which of the following do you agree with?\n",
    "\n",
    "- [ ] Whether you’re using batch gradient descent or mini-batch gradient descent, something is wrong.\n",
    "- [ ] If you’re using mini-batch gradient descent, something is wrong. But if you’re using batch gradient descent, this looks acceptable.\n",
    "- [ ] If you’re using mini-batch gradient descent, this looks acceptable. But if you’re using batch gradient descent, something is wrong.\n",
    "- [ ] Whether you’re using batch gradient descent or mini-batch gradient descent, this looks acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Suppose the temperature in Casablanca over the first three days of January are the same:\n",
    "\n",
    "* Jan 1st: $θ_{1}=10^{o}C$\n",
    "* Jan 2nd: $θ_{2}=10^{o}C$\n",
    "\n",
    "(We used Fahrenheit in lecture, so will use Celsius here in honor of the metric world.)\n",
    "\n",
    "Say you use an exponentially weighted average with $β=0.5$ to track the temperature: $v_{0}=0$, $v_{t}=βv_{t}−1+(1−β)θ_{t}$. If $v_{2}$ is the value computed after day 2 without bias correction, and $v^{corrected}_{2}$ is the value you compute with bias correction. What are these values? (You might be able to do this without a calculator, but you don't actually need one. Remember what is bias correction doing.)\n",
    "\n",
    "\n",
    "- [ ] $v_{2}=7.5$, $v^{corrected}_{2}=7.5$\n",
    "- [ ] $v_{2}2=10$, $v^{corrected}_{2}=10$\n",
    "- [ ] $v_{2}=7.5$, $v^{corrected}_{2}=10$\n",
    "- [ ] $v_{2}=10$, $v^{corrected}_{2}=7.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Which of these is NOT a good learning rate decay scheme? Here, $t$ is the epoch number.\n",
    "\n",
    "- [ ] $α=e^{t}α_{0}$ \n",
    "- [ ] $α=\\frac{1}{\\sqrt{t}}α_{0}$ \n",
    "- [ ] $α=0.95^{t}α_{0}$ \n",
    "- [ ] $α=\\frac{1}{{1+2∗t}}α_{0}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. You use an exponentially weighted average on the London temperature dataset. You use the following to track the temperature: $v_{t}=βv_{t}−1+(1−β)θ_{t}$. The red line below was computed using $β=0.9$. What would happen to your red curve as you vary $β$? (Check the two that apply)\n",
    "\n",
    "<img src=\"images/W0boqHgrEee6mw7xN92yoA_3a1f4052dc56969b5d7da4024a46836d_temp.png\"  alt=\"Figure 2\" style=\"width: 50%;\">\n",
    "\n",
    "- [ ] Decreasing $β$ will shift the red line slightly to the right.\n",
    "- [ ] Increasing $β$ will shift the red line slightly to the right.\n",
    "- [ ] Decreasing $β$ will create more oscillation within the red line.\n",
    "- [ ] Increasing $β$ will create more oscillations within the red line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Consider this figure:\n",
    "\n",
    "<img src=\"images/fv6gungsEeeJIwrF5BVsIg_6da8c45ffcd4075de23d8e93884937f1_GD.png\"  alt=\"Figure 3\" style=\"width: 50%;\">\n",
    "\n",
    "These plots were generated with gradient descent; with gradient descent with momentum ($β = 0.5$) and gradient descent with momentum ($β = 0.9$). Which curve corresponds to which algorithm?\n",
    "\n",
    "- [ ] (1) is gradient descent. (2) is gradient descent with momentum (small $β$). (3) is gradient descent with momentum (large $β$)\n",
    "- [ ] (1) is gradient descent with momentum (small $β$). (2) is gradient descent. (3) is gradient descent with momentum (large $β$)\n",
    "- [ ] (1) is gradient descent. (2) is gradient descent with momentum (large $β$) . (3) is gradient descent with momentum (small $β$)\n",
    "- [ ] (1) is gradient descent with momentum (small $β$), (2) is gradient descent with momentum (small $β$), (3) is gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Suppose batch gradient descent in a deep network is taking excessively long to find a value of the parameters that achieves a small value for the cost function $J(W[1],b[1],...,W[L],b[L])$. Which of the following techniques could help find parameter values that attain a small value for $J$? (Check all that apply)\n",
    "\n",
    "- [ ] Try mini-batch gradient descent\n",
    "- [ ] Try using Adam\n",
    "- [ ] Try tuning the learning rate $α$\n",
    "- [ ] Try better random initialization for the weights\n",
    "- [ ] Try initializing all the weights to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Which of the following statements about Adam is False?\n",
    "\n",
    "- [ ] Adam should be used with batch gradient computations, not with mini-batches.\n",
    "- [ ] The learning rate hyperparameter $α$ in Adam usually needs to be tuned.\n",
    "- [ ] We usually use “default” values for the hyperparameters $β1$,$β2$ and $ε$ in Adam ($β1=0.9$, $β2=0.999$, $ε=10−8$)\n",
    "- [ ] Adam combines the advantages of RMSProp and momentum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
