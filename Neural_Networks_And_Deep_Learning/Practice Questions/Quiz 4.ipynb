{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 4 - Key concepts on Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is the \"cache\" used for in our implementation of forward propagation and backward propagation?\n",
    "\n",
    "- [ ] It is used to keep track of the hyperparameters that we are searching over, to speed up computation.\n",
    "- [ ] We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.\n",
    "- [ ] We use it to pass variables computed during backward propagation to the corresponding forward propagation step. It contains useful values for forward propagation to compute activations.\n",
    "- [ ] It is used to cache the intermediate values of the cost function during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.\n",
    "\n",
    "Explaination: the \"cache\" records values from the forward propagation units and sends it to the backward propagation units because it is needed to compute the chain rule derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Among the following, which ones are \"hyperparameters\"? (Check all that apply)\n",
    "\n",
    "- [ ] activation values $a^{[l]}$\n",
    "- [ ] weight matrices $W^{[l]}$\n",
    "- [ ] number of iterations\n",
    "- [ ] learning rate $\\alpha$\n",
    "- [ ] number of layers $L$ in the neural network\n",
    "- [ ] bias vectors $b^{[l]}$\n",
    "- [ ] size of the hidden layers $n^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: size of the hidden layers $n^{[l]}$\n",
    "\n",
    "A: number of layers $L$ in the neural network\n",
    "\n",
    "A: learning rate $\\alpha$\n",
    "\n",
    "A: number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Which of the following statements is true?\n",
    "\n",
    "- [ ] The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.\n",
    "- [ ] The earlier layers of a neural network are typically computing more complex features of the input than the deeper layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vectorization allows you to compute forward propagation in an $L$-layer neural network without an explicit for-loop (or any other explicit iterative loop) over the layers $l=1, 2, …,L$. True/False?\n",
    "\n",
    "- [ ] True\n",
    "- [ ] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: False\n",
    "\n",
    "Explaination: Forward propagation propagates the input through the layers, although for shallow networks we may just write all the lines. In a deeper networks, we cannot avoid a for loop to iterate through the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Assume we store the values for $n^{[l]}$ in an array called layers, as follows: `layer_dims = [`$n_x$`, 4,3,2,1]`. So layer 1 has four hidden units, layer 2 has 3 hidden units and so on. Which of the following for-loops will allow you to initialize the parameters for the model?\n",
    "\n",
    "- [ ] \n",
    "```\n",
    "for(i in range(1, len(layer_dims)/2)):\n",
    "    parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01\n",
    "    parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01\n",
    "```\n",
    "\n",
    "- [ ] \n",
    "```\n",
    "for(i in range(1, len(layer_dims)/2)):\n",
    "    parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01\n",
    "    parameter[‘b’ + str(i)] = np.random.randn(layers[i-1], 1) * 0.01\n",
    "```\n",
    "\n",
    "- [ ] \n",
    "```\n",
    "for(i in range(1, len(layer_dims))):\n",
    "    parameter[‘W’ + str(i)] = np.random.randn(layers[i-1], layers[i])) * 0.01\n",
    "    parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01\n",
    "```\n",
    "\n",
    "- [ ] \n",
    "```\n",
    "for(i in range(1, len(layer_dims))):\n",
    "    parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01\n",
    "    parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: \n",
    "```\n",
    "for(i in range(1, len(layer_dims))):\n",
    "  parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01\n",
    "  parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Consider the following neural network.\n",
    "\n",
    "<img src=\"images/cwmw1nrfEeeJIwrF5BVsIg_e9a22da9e380c0350d2dfd47dcf34503_Screen-Shot-2017-08-06-at-12.42.46-PM.png\"  alt=\"Figure 1\" style=\"width: 50%;\">\n",
    "\n",
    "How many layers does this network have?\n",
    "\n",
    "- [ ] The number of layers $L$ is 4. The number of hidden layers is 3.\n",
    "- [ ] The number of layers $L$ is 3. The number of hidden layers is 3.\n",
    "- [ ] The number of layers $L$ is 4. The number of hidden layers is 4.\n",
    "- [ ] The number of layers $L$ is 5. The number of hidden layers is 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The number of layers $L$ is 4. The number of hidden layers is 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. During forward propagation, in the forward function for a layer $l$ you need to know what is the activation function in a layer (Sigmoid, tanh, ReLU, etc.). During backpropagation, the corresponding backward function also needs to know what is the activation function for layer $l$, since the gradient depends on it. True/False?\n",
    "\n",
    "- [ ] True\n",
    "- [ ] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: True\n",
    "    \n",
    "Explaination: During backpropagation, you need to know which activation was used in the forward propagation to compute the correct derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. There are certain functions with the following properties:\n",
    "\n",
    "(i) To compute the function using a shallow network circuit, you will need a large network (where we measure size by the number of logic gates in the network), but (ii) To compute it using a deep network circuit, you need only an exponentially smaller network. True/False?\n",
    "\n",
    "- [ ] True\n",
    "- [ ] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Consider the following 2 hidden layer neural network:\n",
    "\n",
    "Which of the following statements are True? (Check all that apply).\n",
    "\n",
    "- [ ] $W^{[1]}$ will have shape `(4, 4)`\n",
    "- [ ] $b^{[1]}$ will have shape `(4, 1)`\n",
    "- [ ] $W^{[1]}$ will have shape `(3, 4)`\n",
    "- [ ] $b^{[1]}$ will have shape `(3, 1)`\n",
    "- [ ] $W^{[2]}$ will have shape `(3, 4)`\n",
    "- [ ] $b^{[2]}$ will have shape `(1, 1)`\n",
    "- [ ] $W^{[2]}$ will have shape `(3, 1)`\n",
    "- [ ] $b^{[2]}$ will have shape `(3, 1)`\n",
    "- [ ] $W^{[3]}$ will have shape `(3, 1)`\n",
    "- [ ] $b^{[3]}$ will have shape `(1, 1)`\n",
    "- [ ] $W^{[3]}$ will have shape `(1, 3)`\n",
    "- [ ] $b^{[3]}$ will have shape `(3, 1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: $W^{[1]}$ will have shape `(4, 4)`\n",
    "\n",
    "A: $b^{[1]}$ will have shape `(4, 1)`\n",
    "\n",
    "A: $W^{[2]}$ will have shape `(3, 4)`\n",
    "\n",
    "A: $b^{[2]}$ will have shape `(3, 1)`\n",
    "\n",
    "A: $b^{[3]}$ will have shape `(1, 1)`\n",
    "\n",
    "A: $W^{[3]}$ will have shape `(1, 3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Whereas the previous question used a specific network, in the general case what is the dimension of $W^{[l]}$, the weight matrix associated with layer $l$?\n",
    "\n",
    "- [ ] $W^{[l]}$ has shape $(n^{[l−1]},n^{[l]})$\n",
    "- [ ] $W^{[l]}$ has shape $(n^{[l]}, n^{[l−1]})$\n",
    "- [ ] $W^{[l]}$ has shape $(n^{[l+1]}, n^{[l]})$\n",
    "- [ ] $W^{[l]}$ has shape $(n^{[l]}, n^{[l+1]})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: $W^{[l]}$ has shape $(n^{[l]}, n^{[l−1]})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
