{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 16 Quiz - Sequence models & Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Consider using this encoder-decoder model for machine translation.\n",
    "\n",
    "<img src=\"images/qXy2GwVdEeirxApwydYI3g_563d74fd24e481f841070e81da7ee0aa_Screen-Shot-2018-01-29-at-5.33.03-PM.png\"  alt=\"Figure 1\" style=\"width: 50%;\">\n",
    "\n",
    "This model is a “conditional language model” in the sense that the encoder portion (shown in green) is modeling the probability of the input sentence $x$.\n",
    "\n",
    "- [ ] True\n",
    "- [ ] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. In beam search, if you increase the beam width $B$, which of the following would you expect to be true? Check all that apply.\n",
    "\n",
    "- [ ] Beam search will run more slowly.\n",
    "- [ ] Beam search will use up more memory.\n",
    "- [ ] Beam search will generally find better solutions (i.e. do a better job maximizing $P(y \\mid x)$)\n",
    "- [ ] Beam search will converge after fewer steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. In machine translation, if we carry out beam search without using sentence normalization, the algorithm will tend to output overly short translations.\n",
    "\n",
    "- [ ] True\n",
    "- [ ] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Suppose you are building a speech recognition system, which uses an RNN model to map from audio clip $x$ to a text transcript $y$. Your algorithm uses beam search to try to find the value of $y$ that maximizes $P(y \\mid x)$.\n",
    "\n",
    "On a dev set example, given an input audio clip, your algorithm outputs the transcript $\\hat{y}=$ \n",
    "“I'm building an A Eye system in Silly con Valley.”, whereas a human gives a much superior transcript $y^* =$ “I'm building an AI system in Silicon Valley.”\n",
    "\n",
    "According to your model,\n",
    "\n",
    "$P(\\hat{y} \\mid x) = 1.09*10^{-7}$\n",
    "\n",
    "$P(y^* \\mid x) = 7.21*10^{-8}$\n",
    "\n",
    "Would you expect increasing the beam width B to help correct this example?\n",
    "\n",
    "- [ ] No, because $P(y^* \\mid x) \\leq P(\\hat{y} \\mid x)$ indicates the error should be attributed to the RNN rather than to the search algorithm.\n",
    "- [ ] No, because $P(y^* \\mid x) \\leq P(\\hat{y} \\mid x)$ indicates the error should be attributed to the search algorithm rather than to the RNN.\n",
    "- [ ] Yes, because $P(y^* \\mid x) \\leq P(\\hat{y} \\mid x)$ indicates the error should be attributed to the RNN rather than to the search algorithm.\n",
    "- [ ] Yes, because $P(y^* \\mid x) \\leq P(\\hat{y} \\mid x)$ indicates the error should be attributed to the search algorithm rather than to the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Continuing the example from Q4, suppose you work on your algorithm for a few more weeks, and now find that for the vast majority of examples on which your algorithm makes a mistake, $P(y^* \\mid x) > P(\\hat{y} \\mid x)$. This suggest you should focus your attention on improving the search algorithm.\n",
    "\n",
    "- [ ] True.\n",
    "- [ ] False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Consider the attention model for machine translation.\n",
    "\n",
    "<img src=\"images/ZdQLWgVeEei_ZQ6W1G11dA_5ea60f98993ef910d93aaf10c16c4cc9_Screen-Shot-2018-01-29-at-5.38.58-PM.png\"  alt=\"Figure 2\" style=\"width: 50%;\">\n",
    "\n",
    "Further, here is the formula for $α^{<t,t′>}$.\n",
    "\n",
    "<img src=\"images/kBdw_AVeEeiIOArs7r1YtA_da38112b640e4901fbbf692a9e6611be_Screen-Shot-2018-01-29-at-5.39.03-PM.png\"  alt=\"Figure 3\" style=\"width: 50%;\">\n",
    "\n",
    "Which of the following statements about $α^{<t,t′>}$ are true? Check all that apply.\n",
    "\n",
    "- [ ] We expect $α^{<t,t′>}$ to be generally larger for values of $a^{<t′>}$ that are highly relevant to the value the network should output for $y^{<t>}$. (Note the indices in the superscripts.)\n",
    "- [ ] We expect $α^{<t,t′>}$ to be generally larger for values of $a^{<t>}$ that are highly relevant to the value the network should output for $y^{<t′>}$. (Note the indices in the superscripts.)\n",
    "- [ ] $∑_tα^{<t,t′>} = 1$ (Note the summation is over $t$.)\n",
    "- [ ] $∑_{t′}α^{<t,t′>} = 1$ (Note the summation is over $t′$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The network learns where to “pay attention” by learning the values $e^{<t,t′>}$, which are computed using a small neural network:\n",
    "\n",
    "We can't replace $s^{<t-1>}$ with $s^{<t>}$ as an input to this neural network. This is because $s^{<t>}$ depends on $α^{<t,t′>}$ which in turn depends on $e^{<t,t′>}$; so at the time we need to evalute this network, we haven’t computed $s^{<t>}$ yet.\n",
    "\n",
    "- [ ] True\n",
    "- [ ] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compared to the encoder-decoder model shown in Question 1 of this quiz (which does not use an attention mechanism), we expect the attention model to have the greatest advantage when:\n",
    "\n",
    "- [ ] The input sequence length $T_x$ is large.\n",
    "- [ ] The input sequence length $T_x$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Under the CTC model, identical repeated characters not separated by the “blank” character (_) are collapsed. Under the CTC model, what does the following string collapse to?\n",
    "\n",
    "    __c_oo_o_kk___b_ooooo__oo__kkk\n",
    "\n",
    "- [ ] cokbok\n",
    "- [ ] cookbook\n",
    "- [ ] cook book\n",
    "- [ ] coookkboooooookkk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Question 10\n",
    "In trigger word detection, $x^{<t>}$ is:\n",
    "\n",
    "- [ ] Features of the audio (such as spectrogram features) at time $t$.\n",
    "- [ ] The $t$-th input word, represented as either a one-hot vector or a word embedding.\n",
    "- [ ] Whether the trigger word is being said at time $t$.\n",
    "- [ ] Whether someone has just finished saying the trigger word at time $t$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
