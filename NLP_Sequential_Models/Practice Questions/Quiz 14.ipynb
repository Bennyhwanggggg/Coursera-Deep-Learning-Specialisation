{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14 Quiz - Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Suppose your training examples are sentences (sequences of words). Which of the following refers to the $j^{th}$ word in the $i^{th}$ training example?\n",
    "\n",
    "- [ ] $x^{(i)<j>}$\n",
    "- [ ] $x^{<i>(j)}$\n",
    "- [ ] $x^{(j)<i>}$\n",
    "- [ ] $x^{<j>(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Consider this RNN:\n",
    "\n",
    "<img src=\"images/WVhjoPCuEee5Rg5IFJ7l8g_a7b6030c6e5a53b431fee7aaabecd9bd_Screen-Shot-2018-01-03-at-5.48.26-PM.png\"  alt=\"Figure 1\" style=\"width: 50%;\">\n",
    "\n",
    "This specific type of architecture is appropriate when:\n",
    "\n",
    "- [ ] $T_x$ = $T_y$\n",
    "- [ ] $T_x < T_y$\n",
    "- [ ] $T_x > T_y$\n",
    "- [ ] $T_x = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. To which of these tasks would you apply a many-to-one RNN architecture? (Check all that apply).\n",
    "\n",
    "<img src=\"images/K59CdPCvEee7LQrRPHr2wA_4549ad1b1b590371eb3502e158a02447_Screen-Shot-2018-01-03-at-5.54.27-PM.png\"  alt=\"Figure 2\" style=\"width: 50%;\">\n",
    "\n",
    "- [ ] Speech recognition (input an audio clip and output a transcript)\n",
    "- [ ] Sentiment classification (input a piece of text and output a 0/1 to denote positive or negative sentiment)\n",
    "- [ ] Image classification (input an image and output a label)\n",
    "- [ ] Gender recognition from speech (input an audio clip and output a label indicating the speaker’s gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. You are training this RNN language model.\n",
    "\n",
    "<img src=\"images/cxeeLPCvEee7YRLKCWJ4hg_bca1b05c70eece156b470abb2d0f0cad_Screen-Shot-2018-01-03-at-5.56.30-PM.png\"  alt=\"Figure 3\" style=\"width: 50%;\">\n",
    "\n",
    "\n",
    "At the $t^{th}$ time step, what is the RNN doing? Choose the best answer.\n",
    "\n",
    "- [ ] Estimating $P(y^{<1>}, y^{<2>}, …, y^{<t-1>})$\n",
    "- [ ] Estimating $P(y^{<t>})$\n",
    "- [ ] Estimating $P(y^{<t>} \\mid y^{<1>}, y^{<2>}, …, y^{<t-1>})$\n",
    "- [ ] Estimating $P(y^{<t>} \\mid y^{<1>}, y^{<2>}, …, y^{<t>})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. You have finished training a language model RNN and are using it to sample random sentences, as follows:\n",
    "\n",
    "What are you doing at each time step $t$?\n",
    "\n",
    "<img src=\"images/zOkWE_CvEee5Rg5IFJ7l8g_f36533d67eb6590d5bcb7021d88493eb_Screen-Shot-2018-01-03-at-5.58.53-PM.png\"  alt=\"Figure 4\" style=\"width: 50%;\">\n",
    "\n",
    "- [ ] (i) Use the probabilities output by the RNN to pick the highest probability word for that time-step as $\\hat{y}^{<t>}$. (ii) Then pass the ground-truth word from the training set to the next time-step.\n",
    "- [ ] (i) Use the probabilities output by the RNN to randomly sample a chosen word for that time-step as $\\hat{y}^{<t>}$. (ii) Then pass the ground-truth word from the training set to the next time-step.\n",
    "- [ ] (i) Use the probabilities output by the RNN to pick the highest probability word for that time-step as $\\hat{y}^{<t>}$. (ii) Then pass this selected word to the next time-step.\n",
    "- [ ] (i) Use the probabilities output by the RNN to randomly sample a chosen word for that time-step as $\\hat{y}^{<t>}$. (ii) Then pass this selected word to the next time-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. You are training an RNN, and find that your weights and activations are all taking on the value of NaN (“Not a Number”). Which of these is the most likely cause of this problem?\n",
    "\n",
    "- [ ] Vanishing gradient problem.\n",
    "- [ ] Exploding gradient problem.\n",
    "- [ ] ReLU activation function $g(.)$ used to compute $g(z)$, where $z$ is too large.\n",
    "- [ ] Sigmoid activation function $g(.)$ used to compute $g(z)$, where $z$ is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Suppose you are training a LSTM. You have a 10000 word vocabulary, and are using an LSTM with 100-dimensional activations $a^{<t>}$. What is the dimension of $\\Gamma_u$ at each time step?\n",
    "\n",
    "- [ ] 1\n",
    "- [ ] 100\n",
    "- [ ] 300\n",
    "- [ ] 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Here’re the update equations for the GRU.\n",
    "\n",
    "<img src=\"images/y-VsavCwEeeVOQpGYM3DAA_b10afdb5d35702d711338d5b72ce5be7_Screen-Shot-2018-01-03-at-6.05.56-PM.png\"  alt=\"Figure 5\" style=\"width: 50%;\">\n",
    "\n",
    "Alice proposes to simplify the GRU by always removing the $\\Gamma_u$. I.e., setting $\\Gamma_u \n",
    "= 1$. Betty proposes to simplify the GRU by removing the $\\Gamma_r$. I. e., setting $\\Gamma_r = 1$ always. Which of these models is more likely to work without vanishing gradient problems even when trained on very long input sequences?\n",
    "\n",
    "- [ ] Alice’s model (removing $\\Gamma_u$), because if $\\Gamma_r$ $\\approx 0$ for a timestep, the gradient can propagate back through that timestep without much decay.\n",
    "\n",
    "- [ ] Alice’s model (removing $\\Gamma_u$), because if $\\Gamma_r$ $\\approx 1$ for a timestep, the gradient can propagate back through that timestep without much decay.\n",
    "\n",
    "- [ ] Betty’s model (removing $\\Gamma_r$), because if $\\Gamma_u$ $\\approx 0$ for a timestep, the gradient can propagate back through that timestep without much decay.\n",
    "\n",
    "- [ ] Betty’s model (removing $\\Gamma_r$), because if $\\Gamma_u$ $\\approx 1$ for a timestep, the gradient can propagate back through that timestep without much decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Here are the equations for the GRU and the LSTM:\n",
    "\n",
    "<img src=\"images/ZJgnEfCxEeeVOQpGYM3DAA_2552c64114ba9a4a065a54e8e4855b39_Screen-Shot-2018-01-03-at-6.10.24-PM.png\"  alt=\"Figure 6\" style=\"width: 50%;\">\n",
    "\n",
    "From these, we can see that the Update Gate and Forget Gate in the LSTM play a role similar to _______ and ______ in the GRU. What should go in the the blanks?\n",
    "\n",
    "- [ ] $\\Gamma_u$ and $1-\\Gamma_u$\n",
    "- [ ] $\\Gamma_u$ and $\\Gamma_r$ \n",
    "- [ ] $1-\\Gamma_u$ and $\\Gamma_u$\n",
    "- [ ] $\\Gamma_r$ and $\\Gamma_u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. You have a pet dog whose mood is heavily dependent on the current and past few days’ weather. You’ve collected data for the past 365 days on the weather, which you represent as a sequence as $x^{<1>}, …, x^{<365>}$. You’ve also collected data on your dog’s mood, which you represent as $y^{<1>}, …, y^{<365>}$. You’d like to build a model to map from $x \\rightarrow y$. Should you use a Unidirectional RNN or Bidirectional RNN for this problem?\n",
    "\n",
    "- [ ] Bidirectional RNN, because this allows the prediction of mood on day t to take into account more information.\n",
    "- [ ] Bidirectional RNN, because this allows backpropagation to compute more accurate gradients.\n",
    "- [ ] Unidirectional RNN, because the value of $y^{<t>}$ depends only on $x^{<1>}, …, x^{<t>}$, but not on $x^{<t+1>}, …, x^{<365>}$\n",
    "- [ ] Unidirectional RNN, because the value of $y^{<t>}$ depends only on $x^{<t>}$, and not other days’ weather."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
