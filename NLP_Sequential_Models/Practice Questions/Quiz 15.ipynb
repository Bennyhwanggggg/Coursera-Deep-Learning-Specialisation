{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 15 Quiz - Natural Language Processing & Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Suppose you learn a word embedding for a vocabulary of 10000 words. Then the embedding vectors should be 10000 dimensional, so as to capture the full range of variation and meaning in those words.\n",
    "\n",
    "- [ ] True\n",
    "- [ ] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is t-SNE?\n",
    "\n",
    "- [ ] A linear transformation that allows us to solve analogies on word vectors\n",
    "- [ ] A non-linear dimensionality reduction technique\n",
    "- [ ] A supervised learning algorithm for learning word embeddings\n",
    "- [ ] An open-source sequence modeling library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Suppose you download a pre-trained word embedding which has been trained on a huge corpus of text. You then use this word embedding to train an RNN for a language task of recognizing if someone is happy from a short snippet of text, using a small training set.\n",
    "\n",
    "    x (input text)\t               y (happy?)\n",
    "    I'm feeling wonderful today!\t 1\n",
    "    I'm bummed my cat is ill.\t    0\n",
    "    Really enjoying this!\t        1\n",
    "\n",
    "Then even if the word “ecstatic” does not appear in your small training set, your RNN might reasonably be expected to recognize “I’m ecstatic” as deserving a label $y = 1$.\n",
    "\n",
    "- [ ] True\n",
    "- [ ] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Which of these equations do you think should hold for a good word embedding? (Check all that apply)\n",
    "\n",
    "- [ ] $e_{boy} - e_{girl} \\approx e_{brother} - e_{sister}$\n",
    "- [ ] $e_{boy} - e_{girl} \\approx e_{sister} - e_{brother}$\n",
    "- [ ] $e_{boy} - e_{brother} \\approx e_{girl} - e_{sister}$\n",
    "- [ ] $e_{boy} - e_{brother} \\approx e_{sister} - e_{girl}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Let $E$ be an embedding matrix, and let $o_{1234}$ be a one-hot vector corresponding to word 1234. Then to get the embedding of word 1234, why don’t we call $E * o_{1234}$ in Python?\n",
    "\n",
    "- [ ] It is computationally wasteful.\n",
    "- [ ] The correct formula is $E^T* o_{1234}$.\n",
    "- [ ] This doesn’t handle unknown words (<UNK>).\n",
    "- [ ] None of the above: calling the Python snippet as described above is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. When learning word embeddings, we create an artificial task of estimating $P(target \\mid context)$. It is okay if we do poorly on this artificial prediction task; the more important by-product of this task is that we learn a useful set of word embeddings.\n",
    "\n",
    "- [ ] True\n",
    "- [ ] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. In the word2vec algorithm, you estimate $P(t \\mid c)$, where $t$ is the target word and $c$ is a context word. How are $t$ and $c$ chosen from the training set? Pick the best answer.\n",
    "\n",
    "- [ ] $c$ is the sequence of all the words in the sentence before $t$.\n",
    "- [ ] $c$ is a sequence of several words immediately before $t$.\n",
    "- [ ] $c$ and $t$ are chosen to be nearby words.\n",
    "- [ ] $c$ is the one word that comes immediately before $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Suppose you have a 10000 word vocabulary, and are learning 500-dimensional word embeddings. The word2vec model uses the following softmax function:\n",
    "\n",
    "$P(t \\mid c) = \\frac{e^{\\theta_t^T e_c}}{\\sum_{t’=1}^{10000} e^{\\theta_{t’}^Te_c}}$\n",
    "\n",
    "Which of these statements are correct? Check all that apply.\n",
    "\n",
    "- [ ] $\\theta_t$ and $e_c$ are both 500 dimensional vectors.\n",
    "- [ ] $\\theta_t$ and $e_c$ are both 10000 dimensional vectors.\n",
    "- [ ] $\\theta_t$ and $e_c$ are both trained with an optimization algorithm such as Adam or gradient descent.\n",
    "- [ ] After training, we should expect $\\theta_t$ to be very close to $e_c$ when $t$ and $c$ are the same word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Suppose you have a 10000 word vocabulary, and are learning 500-dimensional word embeddings.The GloVe model minimizes this objective:\n",
    "\n",
    "$\\min \\sum_{i=1}^{10,000} \\sum_{j=1}^{10,000} f(X_{ij}) (\\theta_i^T e_j + b_i + b_j’ - log X_{ij})^2$\n",
    "\n",
    "Which of these statements are correct? Check all that apply.\n",
    "\n",
    "- [ ] $\\theta_i$ and $e_j$ should be initialized to 0 at the beginning of training.\n",
    "- [ ] $\\theta_i$ and $e_j$ should be initialized randomly at the beginning of training.\n",
    "- [ ] $X_{ij}$ is the number of times word $i$ appears in the context of word $j$.\n",
    "- [ ] The weighting function $f(.)$ must satisfy $f(0) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Question 10\n",
    "\n",
    "You have trained word embeddings using a text dataset of $m_1$ words. You are considering using these word embeddings for a language task, for which you have a separate labeled dataset of $m_2$ words. Keeping in mind that using word embeddings is a form of transfer learning, under which of these circumstance would you expect the word embeddings to be helpful?\n",
    "\n",
    "- [ ] $m_1$ >> $m_2$\n",
    "- [ ] $m_1$ << $m_2$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
